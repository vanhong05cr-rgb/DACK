# -*- coding: utf-8 -*-
"""MNM

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uhqpfo7gjdL0C-ON833cTVOMctT0taR-
"""

import sqlite3
import requests
from bs4 import BeautifulSoup
import concurrent.futures
import random
import time
import re

# ================= C·∫§U H√åNH =================
DB_NAME = "goodreads_fast.db"
START_YEAR = 2020
END_YEAR = 2025
# MAX_BOOKS_PER_GENRE = 20
MAX_WORKERS = 5  # S·ªë lu·ªìng ch·∫°y song song (ƒë·ª´ng ƒë·ªÉ cao qu√° k·∫ªo b·ªã ch·∫∑n IP)

GENRES = [
    "art", "biography", "business", "chick-lit", "christian", "classics",
    "comics", "contemporary", "cookbooks", "crime", "fantasy", "fiction",
    "graphic-novels", "historical-fiction", "history", "horror",
    "humor", "manga", "memoir", "music", "mystery", "nonfiction",
    "poetry", "psychology", "religion", "romance", "science",
    "science-fiction", "self-help", "suspense", "spirituality",
    "sports", "thriller", "travel", "young-adult"
]

# Headers gi·∫£ l·∫≠p tr√¨nh duy·ªát ƒë·ªÉ tr√°nh b·ªã ch·∫∑n
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept-Language": "en-US,en;q=0.9",
    "Referer": "https://www.goodreads.com/"
}

# ================= DATABASE =================
def init_database():
    conn = sqlite3.connect(DB_NAME)
    cursor = conn.cursor()
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS books (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            genre TEXT,
            title TEXT,
            author TEXT,
            published_year INTEGER,
            rating TEXT,
            book_url TEXT UNIQUE,
            image_url TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    conn.commit()
    conn.close()

def save_book_to_db(book_data):
    """H√†m l∆∞u ƒë·ªôc l·∫≠p ƒë·ªÉ d√πng trong thread"""
    if not book_data: return

    try:
        conn = sqlite3.connect(DB_NAME)
        cursor = conn.cursor()
        cursor.execute('''
            INSERT OR IGNORE INTO books (genre, title, author, published_year, rating, book_url, image_url)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        ''', (
            book_data['genre'], book_data['title'], book_data['author'],
            book_data['year'], book_data['rating'], book_data['url'],
            book_data['image_url']
        ))
        conn.commit()
        conn.close()
        return True
    except Exception as e:
        print(f"L·ªói DB: {e}")
        return False

# ================= C√ÄO D·ªÆ LI·ªÜU (CORE) =================

def get_soup(url):
    """G·ª≠i request v√† tr·∫£ v·ªÅ BeautifulSoup object"""
    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        if response.status_code == 200:
            return BeautifulSoup(response.text, 'html.parser')
        elif response.status_code == 403:
            print(f"‚ö†Ô∏è B·ªã ch·∫∑n (403) t·∫°i {url}. ƒêang ngh·ªâ 5s...")
            time.sleep(5)
    except Exception as e:
        print(f"L·ªói k·∫øt n·ªëi {url}: {e}")
    return None

def extract_year(text):
    if not text: return None
    match = re.search(r'\d{4}', text)
    return int(match.group(0)) if match else None

def process_book(url, genre):
    """H√†m x·ª≠ l√Ω chi ti·∫øt 1 cu·ªën s√°ch (ch·∫°y trong lu·ªìng ri√™ng)"""
    # Random sleep nh·∫π ƒë·ªÉ gi·∫£m t·∫£i server
    time.sleep(random.uniform(0.5, 1.5))

    soup = get_soup(url)
    if not soup: return None

    data = {
        "genre": genre,
        "url": url,
        "title": "Unknown",
        "author": "Unknown",
        "year": None,
        "rating": "N/A",
        "image_url": "N/A"
    }

    try:
        # 1. Title
        title_tag = soup.find("h1", {"data-testid": "bookTitle"})
        if title_tag: data["title"] = title_tag.get_text(strip=True)

        # 2. Author
        author_tag = soup.find("span", {"data-testid": "name"})
        if author_tag: data["author"] = author_tag.get_text(strip=True)

        # 3. Year
        pub_tag = soup.find("p", {"data-testid": "publicationInfo"})
        if pub_tag: data["year"] = extract_year(pub_tag.get_text())

        # 4. Rating
        rating_div = soup.find("div", class_="RatingStatistics__rating")
        if rating_div: data["rating"] = rating_div.get_text(strip=True)

        # 5. Image
        img_tag = soup.find("img", class_="ResponsiveImage")
        if img_tag: data["image_url"] = img_tag.get("src")

        # L·ªçc nƒÉm
        if data["year"] and START_YEAR <= data["year"] <= END_YEAR:
            print(f"‚úÖ {genre.upper()}: {data['title']} ({data['year']})")
            save_book_to_db(data)
            return data
        else:
            # print(f"‚ùå Sai nƒÉm: {data['year']}") # B·ªè comment n·∫øu mu·ªën debug
            pass

    except Exception as e:
        print(f"L·ªói parse s√°ch {url}: {e}")

    return None

def get_book_links_from_genre(genre):
    """L·∫•y danh s√°ch link s√°ch t·ª´ trang New Releases"""
    url = f"https://www.goodreads.com/genres/new_releases/{genre}"
    soup = get_soup(url)
    links = []
    if soup:
        # T√¨m c√°c th·∫ª a ch·ª©a link s√°ch (c·∫•u tr√∫c th∆∞·ªùng th·∫•y ·ªü trang list)
        cover_wrappers = soup.select("div.coverWrapper a")
        for a in cover_wrappers:
            href = a.get('href')
            if href and "/book/show/" in href:
                full_link = "https://www.goodreads.com" + href.split('?')[0] # B·ªè tham s·ªë query th·ª´a
                links.append(full_link)

    # L·ªçc tr√πng v√† gi·ªõi h·∫°n s·ªë l∆∞·ª£ng
    return list(set(links))

# ================= MAIN =================

def main():
    init_database()
    print("üöÄ B·∫Øt ƒë·∫ßu ch·∫ø ƒë·ªô T·ªêC ƒê·ªò CAO (Requests + Multi-threading)...")

    for genre in GENRES:
        print(f"\n--- üìÇ ƒêang qu√©t th·ªÉ lo·∫°i: {genre} ---")

        # 1. L·∫•y danh s√°ch link (Ch·∫°y ƒë∆°n lu·ªìng v√¨ ch·ªâ c√≥ 1 trang list)
        links = get_book_links_from_genre(genre)
        print(f"-> T√¨m th·∫•y {len(links)} s√°ch ti·ªÅm nƒÉng. B·∫Øt ƒë·∫ßu t·∫£i chi ti·∫øt...")

        # 2. V√†o chi ti·∫øt t·ª´ng s√°ch (Ch·∫°y ƒêA LU·ªíNG)
        # S·ª≠ d·ª•ng ThreadPoolExecutor ƒë·ªÉ ch·∫°y nhi·ªÅu request c√πng l√∫c
        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            # Submit c√°c t√°c v·ª• v√†o pool
            futures = [executor.submit(process_book, link, genre) for link in links]

            # ƒê·ª£i c√°c t√°c v·ª• ho√†n th√†nh
            concurrent.futures.wait(futures)

    print("\n‚úÖ HO√ÄN T·∫§T TO√ÄN B·ªò!")

if __name__ == "__main__":
    main()